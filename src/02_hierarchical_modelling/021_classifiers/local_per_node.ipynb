{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d28f0-e1a0-4930-8db6-1c2568a3b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff7772f-10aa-406f-bf91-8a3d8fea79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, dataset):\n",
    "    \"\"\"\n",
    "    Load the data from a Parquet file, encode the target variable, and split the data into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): Name of the file to load (without '.parquet' extension and path).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training labels.\n",
    "    - X_val (DataFrame): Validation features.\n",
    "    - y_val (Series): Validation labels.\n",
    "    \"\"\"\n",
    "    data_path = f'../../../data/features/{dataset}/{file_name}.parquet'\n",
    "    data = pd.read_parquet(data_path)\n",
    "    \n",
    "    expanded_lineages = []\n",
    "    for lineage in data[\"Target\"]:\n",
    "        output = split_and_replace(lineage, mapping)\n",
    "        output_str = \"\".join(output)\n",
    "        expanded_lineages.append(output_str)\n",
    "    \n",
    "    data[\"Target\"] = expanded_lineages\n",
    "    data = data[~data[\"Target\"].astype(str).str.contains(r\"\\[|\\*\")]\n",
    "\n",
    "    X_train = data[data['Train'] == 0].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_train = data[data['Train'] == 0]['Target']\n",
    "    X_val = data[data['Train'] == 1].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_val = data[data['Train'] == 1]['Target']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def split_string(input_string):\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "\n",
    "    for char in input_string:\n",
    "        if char.isalnum():\n",
    "            current_segment += char  # Append alphanumeric characters to the current segment\n",
    "        elif char.isspace():\n",
    "            continue  # Skip whitespace characters\n",
    "        else:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment)  # Add the current segment to the list\n",
    "                current_segment = \"\"\n",
    "            segments.append(char)  # Add the delimiter as a separate segment\n",
    "\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)  # Add the last segment if any\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_and_replace(input_string, mapping):\n",
    "    segments = split_string(input_string)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(segments):\n",
    "        segment = segments[i]\n",
    "        if segment in mapping:\n",
    "            # Replace the segment with its mapped value and split it\n",
    "            new_segments = split_string(mapping[segment])\n",
    "            segments = segments[:i] + new_segments + segments[i+1:]\n",
    "            # Do not increment i, so the loop will check the new segments on the next iteration\n",
    "        else:\n",
    "            # Only increment i if no replacement was done\n",
    "            i += 1\n",
    "\n",
    "    return segments\n",
    "\n",
    "def extract_nodes(y_train):\n",
    "    nodes = set()\n",
    "    for path in y_train:\n",
    "        parts = path.split('.')\n",
    "        for i in range(1, len(parts) + 1):\n",
    "            nodes.add('.'.join(parts[:i]))\n",
    "    return nodes\n",
    "\n",
    "def is_descendant(node, label):\n",
    "    \"\"\"\n",
    "    Check if the label is a descendant of the node.\n",
    "\n",
    "    Parameters:\n",
    "    - node (str): The node we're checking against (e.g., 'B.1').\n",
    "    - label (str): The label being checked (e.g., 'B.1.135').\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if label is a descendant of node, False otherwise.\n",
    "    \"\"\"\n",
    "    if label == node:\n",
    "        return False  # A node is not a descendant of itself\n",
    "    if label.startswith(node):\n",
    "        return label[len(node)] == '.'  # Ensures that 'B.11' is not a descendant of 'B.1'\n",
    "    return False\n",
    "\n",
    "# def get_siblings(node, all_nodes):\n",
    "#     parent = '.'.join(node.split('.')[:-1])\n",
    "#     siblings = [n for n in all_nodes if n.startswith(parent) and n != node]\n",
    "#     return siblings\n",
    "\n",
    "# def get_descendants(node, all_nodes):\n",
    "#     return [n for n in all_nodes if n.startswith(node) and n != node]\n",
    "\n",
    "# def get_siblings_and_their_descendants(node, all_nodes):\n",
    "#     siblings = get_siblings(node, all_nodes)\n",
    "#     siblings_descendants = []\n",
    "#     for sibling in siblings:\n",
    "#         siblings_descendants.extend(get_descendants(sibling, all_nodes))\n",
    "#     return siblings + siblings_descendants\n",
    "\n",
    "def is_ancestor(ancestor, label):\n",
    "    return ancestor != label and label.startswith(ancestor)\n",
    "\n",
    "def exclusive_policy(node, y_train):\n",
    "    \"\"\"Exclusive policy: Instances exactly matching the node are positive; others are negative.\"\"\"\n",
    "    return y_train.apply(lambda x: 1 if x == node else -1)\n",
    "\n",
    "def less_exclusive_policy(node, y_train):\n",
    "    \"\"\"Less exclusive policy: Assign 1 to exact matches, 0 to descendents and -1 to everything else.\"\"\"\n",
    "    labels = pd.Series([0] * len(y_train), index=y_train.index)  # Initialize with zeros (to be excluded)\n",
    "\n",
    "    # Assign 1 to samples exactly matching the node\n",
    "    labels[y_train.apply(lambda x: x == node)] = 1\n",
    "\n",
    "    # Assign -1 to samples that are clearly not the node and not its descendants\n",
    "    # This assumes you have a function is_descendant(node, x) available\n",
    "    labels[y_train.apply(lambda x: not is_descendant(node, x) and x != node)] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def less_inclusive_policy(node, y_train):\n",
    "    labels = pd.Series([0] * len(y_train), index=y_train.index)\n",
    "\n",
    "    # Assigning positive labels to the node or its descendants\n",
    "    labels[y_train.apply(lambda x: x == node or x.startswith(node + '.'))] = 1\n",
    "\n",
    "    # Assigning negative labels to non-descendants and non-node\n",
    "    labels[y_train.apply(lambda x: not x.startswith(node))] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "def inclusive_policy(node, y_train):\n",
    "    labels = pd.Series([0] * len(y_train), index=y_train.index)\n",
    "\n",
    "    # Assigning positive labels to the node or its descendants\n",
    "    labels[y_train.apply(lambda x: x == node or x.startswith(node + '.'))] = 1\n",
    "\n",
    "    # Assigning negative labels excluding the node, its descendants, and ancestors\n",
    "    labels[y_train.apply(lambda x: not x.startswith(node) and not is_ancestor(node, x))] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "def my_policy(node, y_train):\n",
    "    return y_train.apply(lambda x: int(node == '.'.join(x.split('.')[:len(node.split('.'))]))).replace(0, -1)\n",
    "\n",
    "\n",
    "# def siblings_policy(node, y_train):\n",
    "#     all_nodes = y_train.unique()\n",
    "#     siblings_and_their_descendants = get_siblings_and_their_descendants(node, all_nodes)\n",
    "\n",
    "#     labels = pd.Series([0] * len(y_train), index=y_train.index)\n",
    "\n",
    "#     # Assigning positive labels to the node or its descendants\n",
    "#     labels[y_train.apply(lambda x: x == node or x.startswith(node + '.'))] = 1\n",
    "\n",
    "#     # Assigning negative labels for sibling nodes and their descendants\n",
    "#     labels[y_train.isin(siblings_and_their_descendants)] = -1\n",
    "\n",
    "#     return labels\n",
    "\n",
    "# def exclusive_siblings_policy(node, y_train):\n",
    "#     all_nodes = y_train.unique()\n",
    "#     siblings = get_siblings(node, all_nodes)\n",
    "\n",
    "#     labels = pd.Series([0] * len(y_train), index=y_train.index)\n",
    "\n",
    "#     # Assigning positive labels for the node\n",
    "#     labels[y_train == node] = 1\n",
    "\n",
    "#     # Assigning negative labels for sibling nodes\n",
    "#     labels[y_train.isin(siblings)] = -1\n",
    "\n",
    "#     return labels\n",
    "\n",
    "\n",
    "def get_labels_by_policy(node, y_train, policy):\n",
    "    \"\"\"Wrapper function to get labels by chosen policy.\"\"\"\n",
    "    if policy == 'exclusive':\n",
    "        return exclusive_policy(node, y_train)\n",
    "    elif policy == 'less_exclusive':\n",
    "        return less_exclusive_policy(node, y_train)\n",
    "    elif policy == 'less_inclusive':\n",
    "        return less_inclusive_policy(node, y_train)\n",
    "    elif policy == 'inclusive':\n",
    "        return inclusive_policy(node, y_train)\n",
    "    elif policy == 'siblings':\n",
    "        return siblings_policy(node, y_train)\n",
    "    elif policy == 'exclusive_siblings':\n",
    "        return exclusive_siblings_policy(node, y_train)\n",
    "    elif policy == 'mine':\n",
    "        return my_policy(node, y_train)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be97f52a-1fe6-4c8b-9b5d-b2250f9cdc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alias_key.json', 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    if isinstance(value, list):\n",
    "        mapping[key] = '[' + ', '.join(value) + ']'\n",
    "        \n",
    "file_names = [os.path.splitext(os.path.basename(file))[0] for file in glob.glob(\"../../../data/features/SARS/*.parquet\")]\n",
    "dataset = \"SARS\"\n",
    "file_name = \"FCGR_remove_256\"\n",
    "# print(file_names)\n",
    "\n",
    "X_train, y_train, X_val, y_val = load_data(file_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710fc645-af5c-4663-853a-97a9f373fded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training classifiers: 100%|██████████| 150/150 [02:40<00:00,  1.07s/it]\n",
      "Predicting: 100%|██████████| 150/150 [01:28<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_nodes = extract_nodes(y_train)\n",
    "labeling_policy = 'inclusive'\n",
    "\n",
    "# Initializing a dictionary to hold the binary classifiers for each node\n",
    "classifiers = defaultdict(lambda: RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "\n",
    "# Training a binary classifier for each node according to the chosen policy\n",
    "for node in tqdm(unique_nodes, desc=\"Training classifiers\"):\n",
    "    labels = get_labels_by_policy(node, y_train, labeling_policy)\n",
    "    \n",
    "    # Filter out samples that should not be considered for training\n",
    "    X_train_filtered = X_train[labels != 0]\n",
    "    y_train_filtered = labels[labels != 0]\n",
    "    \n",
    "    if len(y_train_filtered.value_counts()) == 1:\n",
    "        print(f\"{node}: {y_train_filtered.value_counts()}\")\n",
    "\n",
    "    if len(y_train_filtered) > 0:\n",
    "        clf = classifiers[node]\n",
    "        clf.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Example of how to predict (assuming X_val is defined)\n",
    "predictions = {node: clf.predict_proba(X_val)[:, 1] for node, clf in tqdm(classifiers.items(), desc=\"Predicting\")}\n",
    "predictions_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244bc99f-5917-4c3c-a06a-e68f2d3c888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(f'predictions/LPN_{labeling_policy}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
