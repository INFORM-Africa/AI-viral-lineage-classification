{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d28f0-e1a0-4930-8db6-1c2568a3b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff7772f-10aa-406f-bf91-8a3d8fea79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, dataset):\n",
    "    \"\"\"\n",
    "    Load the data from a Parquet file, encode the target variable, and split the data into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): Name of the file to load (without '.parquet' extension and path).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training labels.\n",
    "    - X_val (DataFrame): Validation features.\n",
    "    - y_val (Series): Validation labels.\n",
    "    \"\"\"\n",
    "    data_path = f'../../../data/features/{dataset}/{file_name}.parquet'\n",
    "    data = pd.read_parquet(data_path)\n",
    "    \n",
    "    expanded_lineages = []\n",
    "    for lineage in data[\"Target\"]:\n",
    "        output = split_and_replace(lineage, mapping)\n",
    "        output_str = \"\".join(output)\n",
    "        expanded_lineages.append(output_str)\n",
    "    \n",
    "    data[\"Target\"] = expanded_lineages\n",
    "    data = data[~data[\"Target\"].astype(str).str.contains(r\"\\[|\\*\")]\n",
    "\n",
    "    X_train = data[data['Train'] == 0].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_train = data[data['Train'] == 0]['Target']\n",
    "    X_val = data[data['Train'] == 1].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_val = data[data['Train'] == 1]['Target']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def extract_nodes(y_train):\n",
    "    nodes = set()\n",
    "    for path in y_train:\n",
    "        parts = path.split('.')\n",
    "        for i in range(1, len(parts) + 1):\n",
    "            nodes.add('.'.join(parts[:i]))\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64905ea0-4fc6-4fd0-b43b-0989fe939bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(input_string):\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "\n",
    "    for char in input_string:\n",
    "        if char.isalnum():\n",
    "            current_segment += char  # Append alphanumeric characters to the current segment\n",
    "        elif char.isspace():\n",
    "            continue  # Skip whitespace characters\n",
    "        else:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment)  # Add the current segment to the list\n",
    "                current_segment = \"\"\n",
    "            segments.append(char)  # Add the delimiter as a separate segment\n",
    "\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)  # Add the last segment if any\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_and_replace(input_string, mapping):\n",
    "    segments = split_string(input_string)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(segments):\n",
    "        segment = segments[i]\n",
    "        if segment in mapping:\n",
    "            # Replace the segment with its mapped value and split it\n",
    "            new_segments = split_string(mapping[segment])\n",
    "            segments = segments[:i] + new_segments + segments[i+1:]\n",
    "            # Do not increment i, so the loop will check the new segments on the next iteration\n",
    "        else:\n",
    "            # Only increment i if no replacement was done\n",
    "            i += 1\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be97f52a-1fe6-4c8b-9b5d-b2250f9cdc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alias_key.json', 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    if isinstance(value, list):\n",
    "        mapping[key] = '[' + ', '.join(value) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e662c1-5c24-4eac-add9-9e55486258a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [os.path.splitext(os.path.basename(file))[0] for file in glob.glob(\"../../../data/features/SARS/*.parquet\")]\n",
    "dataset = \"SARS\"\n",
    "file_name = \"FCGR_remove_256\"\n",
    "# print(file_names)\n",
    "\n",
    "X_train, y_train, X_val, y_val = load_data(file_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea773327-3954-41f7-a065-bd710570663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef2838-f77e-4ee4-884b-2a741117ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_level = max(len(node.split('.')) for node in y_train)\n",
    "y_train_e = [label + '.e' if len(label.split('.')) < max_level else label for label in y_train]\n",
    "classifiers = defaultdict(lambda: RandomForestClassifier(random_state=42, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6672ee5-fcf5-4c89-a01e-e04792d04066",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = extract_nodes(y_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb78e44-6e9a-46ac-b05f-a82b46f327df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_by_policy(level, X_train, y_train):\n",
    "    # Filter out samples where the label ends before the current level\n",
    "    valid_indices = [i for i, sample in enumerate(y_train) if len(sample.split('.')) > level]\n",
    "    \n",
    "    # Ensure X_train is accessed correctly\n",
    "    if isinstance(X_train, pd.DataFrame) or isinstance(X_train, pd.Series):\n",
    "        X_train_filtered = X_train.iloc[valid_indices]\n",
    "    else:\n",
    "        # Assuming X_train is a NumPy array or similar\n",
    "        X_train_filtered = X_train[valid_indices]\n",
    "\n",
    "    y_train_filtered = ['.'.join(y_train[i].split('.')[:level + 1]) for i in valid_indices]\n",
    "\n",
    "    return X_train_filtered, np.array(y_train_filtered)\n",
    "\n",
    "for level in range(max_level):\n",
    "    # Generate labels and filter X_train for the current level\n",
    "    X_train_level, y_train_level = get_labels_by_policy(level, X_train, y_train_e)\n",
    "    \n",
    "    print(np.unique(y_train_level))\n",
    "\n",
    "    # Train a single classifier for this level\n",
    "    clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train_level, y_train_level)  # Use the filtered and correctly labeled training data\n",
    "\n",
    "    # Store the trained classifier\n",
    "    classifiers[level] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971a539-46fd-4c13-a005-456abf254b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the DataFrames, one for each level\n",
    "level_predictions_dfs = []\n",
    "\n",
    "for level in range(max_level):\n",
    "    # Determine the current level nodes (the labels for this level)\n",
    "    \n",
    "    # Ensure the classifier for the current level exists\n",
    "    if level in classifiers:\n",
    "        clf = classifiers[level]\n",
    "        \n",
    "        # Get the predicted probabilities for the validation set\n",
    "        probabilities = clf.predict_proba(X_val)\n",
    "        \n",
    "        # Retrieve the class labels\n",
    "        class_labels = clf.classes_\n",
    "\n",
    "        # Create a DataFrame for the current level's probabilities\n",
    "        level_df = pd.DataFrame(probabilities, columns=class_labels)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        level_predictions_dfs.append(level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3664a-805e-4b42-a798-a7d6f1461a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.concat(level_predictions_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebb192-a279-4c45-ad02-4d54d8d28509",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c3b1b-4005-4f6c-b3c4-e12229ff90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_levels(labels):\n",
    "#     max_level = max(len(label.split('.')) for label in labels)\n",
    "#     level_data = {level: [] for level in range(max_level)}\n",
    "    \n",
    "#     for label in labels:\n",
    "#         parts = label.split('.')\n",
    "#         for level in range(max_level):\n",
    "#             if level < len(parts):\n",
    "#                 level_data[level].append('.'.join(parts[:level+1]))\n",
    "#             else:\n",
    "#                 # If the current label has fewer levels, repeat the last available part\n",
    "#                 level_data[level].append(level_data[level-1][-1])\n",
    "    \n",
    "#     return level_data\n",
    "\n",
    "# # Extract levels from y_train and y_val\n",
    "# train_levels = extract_levels(y_train)\n",
    "# val_levels = extract_levels(y_val)\n",
    "\n",
    "# # Now, let's ensure that the length matches\n",
    "# for level in train_levels:\n",
    "#     assert len(train_levels[level]) == len(y_train), f\"Length mismatch in training level {level}\"\n",
    "#     assert len(val_levels[level]) == len(y_val), f\"Length mismatch in validation level {level}\"\n",
    "    \n",
    "# def create_confidence_df(models, X_val, train_levels):\n",
    "#     \"\"\"\n",
    "#     Creates a DataFrame with confidence scores for each node.\n",
    "\n",
    "#     :param models: Dictionary of trained models for each level.\n",
    "#     :param X_val: Validation features.\n",
    "#     :param train_levels: Dictionary containing the labels for each level used during training.\n",
    "#     :return: DataFrame with confidence scores.\n",
    "#     \"\"\"\n",
    "#     # Dictionary to collect confidence scores\n",
    "#     confidences = {}\n",
    "\n",
    "#     for level, model in models.items():\n",
    "#         # Predict the probability for each class\n",
    "#         probas = model.predict_proba(X_val)\n",
    "\n",
    "#         # Match the predicted probabilities to the corresponding nodes\n",
    "#         for idx, label in enumerate(model.classes_):\n",
    "#             # The label here corresponds to the node\n",
    "#             # We take the probability of the class being present (assuming binary classification, index 1)\n",
    "#             confidences[label] = probas[:, idx]\n",
    "\n",
    "#     # Create the DataFrame from the dictionary\n",
    "#     confidences_df = pd.DataFrame(confidences)\n",
    "\n",
    "#     return confidences_df\n",
    "\n",
    "# # Initialize models, predictions, and accuracies\n",
    "# models = {}\n",
    "# predictions = {}\n",
    "# val_accuracies = {}\n",
    "\n",
    "# # Unique labels (classes) for each level\n",
    "# level_labels = {level: set(labels) for level, labels in train_levels.items()}\n",
    "\n",
    "# for level in train_levels:\n",
    "#     model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "#     model.fit(X_train, train_levels[level])\n",
    "#     models[level] = model\n",
    "    \n",
    "#     # Validate the model\n",
    "#     y_pred = model.predict(X_val)\n",
    "#     predictions[level] = y_pred\n",
    "#     val_accuracies[level] = accuracy_score(val_levels[level], y_pred)\n",
    "    \n",
    "#     # Create the confidence DataFrame\n",
    "# confidence_df = create_confidence_df(models, X_val, train_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6748b-aa27-491e-8768-75d99778b596",
   "metadata": {},
   "source": [
    "### Top Down to Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6462538-7ffc-4e64-94e8-dd375bfb71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_children(predictions_df):\n",
    "    # Extract column names that don't contain '.' indicating they are root children\n",
    "    return [col for col in predictions_df.columns if '.' not in col]\n",
    "\n",
    "def get_children(node, predictions_df):\n",
    "    if node:  # If the node is specified, find its children\n",
    "        prefix = node + '.'\n",
    "        return [col for col in predictions_df.columns if col.startswith(prefix) and col.count('.') == node.count('.') + 1]\n",
    "    else:  # If no node is specified, return root children\n",
    "        return get_root_children(predictions_df)\n",
    "    \n",
    "def navigate_tree_top_down(predictions_df):\n",
    "    results_df = pd.DataFrame(index=predictions_df.index)\n",
    "\n",
    "    for index, row in predictions_df.iterrows():\n",
    "        # Start with root children\n",
    "        current_level = 0\n",
    "        current_nodes = get_root_children(predictions_df)\n",
    "        predicted_path = []\n",
    "        \n",
    "        # Iterate down the levels of the hierarchy\n",
    "        while current_nodes:\n",
    "            # Select the node with the maximum confidence\n",
    "            max_conf_node = max(current_nodes, key=lambda node: row[node])\n",
    "            max_confidence = row[max_conf_node]\n",
    "\n",
    "            # Append the selected node to the path\n",
    "            predicted_path.append(max_conf_node)\n",
    "            \n",
    "            # Proceed to the next level if the current node is not a leaf\n",
    "            current_nodes = get_children(max_conf_node, predictions_df)\n",
    "            current_level += 1\n",
    "\n",
    "        # Assign the last node in the predicted path as the final prediction\n",
    "        results_df.at[index, 'prediction'] = predicted_path[-1] if predicted_path else None\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc52be-e98b-4735-96a9-41b84280e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = navigate_tree_top_down(predictions_df)['prediction'].str.replace(\".e\", \"\", regex=False)\n",
    "\n",
    "# Evaluate the overall performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')  # Consider weighted if class imbalance is present\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5342b28-a93e-4f3c-88cf-f91fbfada708",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('predictions/LPL.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
