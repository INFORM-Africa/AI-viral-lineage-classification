{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d28f0-e1a0-4930-8db6-1c2568a3b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff7772f-10aa-406f-bf91-8a3d8fea79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, dataset):\n",
    "    \"\"\"\n",
    "    Load the data from a Parquet file, encode the target variable, and split the data into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): Name of the file to load (without '.parquet' extension and path).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training labels.\n",
    "    - X_val (DataFrame): Validation features.\n",
    "    - y_val (Series): Validation labels.\n",
    "    \"\"\"\n",
    "    data_path = f'../../../data/features/{dataset}/{file_name}.parquet'\n",
    "    data = pd.read_parquet(data_path)\n",
    "    \n",
    "    expanded_lineages = []\n",
    "    for lineage in data[\"Target\"]:\n",
    "        output = split_and_replace(lineage, mapping)\n",
    "        output_str = \"\".join(output)\n",
    "        expanded_lineages.append(output_str)\n",
    "    \n",
    "    data[\"Target\"] = expanded_lineages\n",
    "    data = data[~data[\"Target\"].astype(str).str.contains(r\"\\[|\\*\")]\n",
    "\n",
    "    X_train = data[data['Train'] == 0].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_train = data[data['Train'] == 0]['Target']\n",
    "    X_val = data[data['Train'] == 1].drop(columns=[\"Target\", \"Train\"])\n",
    "    y_val = data[data['Train'] == 1]['Target']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def extract_nodes(y_train):\n",
    "    nodes = set()\n",
    "    for path in y_train:\n",
    "        parts = path.split('.')\n",
    "        for i in range(1, len(parts) + 1):\n",
    "            nodes.add('.'.join(parts[:i]))\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64905ea0-4fc6-4fd0-b43b-0989fe939bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(input_string):\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "\n",
    "    for char in input_string:\n",
    "        if char.isalnum():\n",
    "            current_segment += char  # Append alphanumeric characters to the current segment\n",
    "        elif char.isspace():\n",
    "            continue  # Skip whitespace characters\n",
    "        else:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment)  # Add the current segment to the list\n",
    "                current_segment = \"\"\n",
    "            segments.append(char)  # Add the delimiter as a separate segment\n",
    "\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)  # Add the last segment if any\n",
    "\n",
    "    return segments\n",
    "\n",
    "def split_and_replace(input_string, mapping):\n",
    "    segments = split_string(input_string)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(segments):\n",
    "        segment = segments[i]\n",
    "        if segment in mapping:\n",
    "            # Replace the segment with its mapped value and split it\n",
    "            new_segments = split_string(mapping[segment])\n",
    "            segments = segments[:i] + new_segments + segments[i+1:]\n",
    "            # Do not increment i, so the loop will check the new segments on the next iteration\n",
    "        else:\n",
    "            # Only increment i if no replacement was done\n",
    "            i += 1\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be97f52a-1fe6-4c8b-9b5d-b2250f9cdc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alias_key.json', 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    if isinstance(value, list):\n",
    "        mapping[key] = '[' + ', '.join(value) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e662c1-5c24-4eac-add9-9e55486258a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [os.path.splitext(os.path.basename(file))[0] for file in glob.glob(\"../../../data/features/SARS/*.parquet\")]\n",
    "dataset = \"SARS\"\n",
    "file_name = \"FCGR_remove_256\"\n",
    "# print(file_names)\n",
    "\n",
    "X_train, y_train, X_val, y_val = load_data(file_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ef2838-f77e-4ee4-884b-2a741117ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_level = max(len(node.split('.')) for node in y_train)\n",
    "y_train_e = [label + '.e' if len(label.split('.')) < max_level else label for label in y_train]\n",
    "classifiers = defaultdict(lambda: RandomForestClassifier(random_state=42, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6672ee5-fcf5-4c89-a01e-e04792d04066",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = extract_nodes(y_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb78e44-6e9a-46ac-b05f-a82b46f327df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A', 'B'}\n",
      "{'B.e', 'A.e', 'B.1', 'B.40'}\n",
      "{'B.1.8', 'B.1.462', 'B.1.381', 'B.1.417', 'B.1.351', 'B.1.237', 'B.1.525', 'B.1.467', 'B.1.638', 'B.40.e', 'B.1.140', 'B.1.617', 'B.1.1', 'B.1.e'}\n",
      "{'B.1.1.84', 'B.1.1.448', 'B.1.1.386', 'B.1.417.e', 'B.1.381.e', 'B.1.1.487', 'B.1.1.117', 'B.1.1.382', 'B.1.237.e', 'B.1.638.e', 'B.1.1.40', 'B.1.1.7', 'B.1.1.459', 'B.1.1.383', 'B.1.1.10', 'B.1.1.54', 'B.1.1.1', 'B.1.351.e', 'B.1.8.e', 'B.1.1.57', 'B.1.462.e', 'B.1.1.456', 'B.1.467.e', 'B.1.1.273', 'B.1.1.53', 'B.1.140.e', 'B.1.617.2', 'B.1.1.412', 'B.1.1.99', 'B.1.1.e', 'B.1.1.529', 'B.1.1.528', 'B.1.617.e', 'B.1.1.62', 'B.1.1.254', 'B.1.1.34', 'B.1.1.507', 'B.1.1.56', 'B.1.525.e', 'B.1.1.318'}\n",
      "{'B.1.1.529.4', 'B.1.1.40.e', 'B.1.1.383.e', 'B.1.1.459.e', 'B.1.1.529.3', 'B.1.617.2.32', 'B.1.1.56.e', 'B.1.617.2.120', 'B.1.617.2.19', 'B.1.1.34.e', 'B.1.1.1.6', 'B.1.1.318.e', 'B.1.1.487.e', 'B.1.617.2.122', 'B.1.617.2.116', 'B.1.1.1.1', 'B.1.1.382.e', 'B.1.1.529.5', 'B.1.1.507.e', 'B.1.1.456.e', 'B.1.1.254.e', 'B.1.617.2.36', 'B.1.617.2.109', 'B.1.1.529.e', 'B.1.1.117.e', 'B.1.1.1.2', 'B.1.1.84.e', 'B.1.1.54.e', 'B.1.617.2.46', 'B.1.1.53.e', 'B.1.1.273.e', 'B.1.617.2.38', 'B.1.617.2.107', 'B.1.1.528.e', 'B.1.617.2.99', 'B.1.1.1.9', 'B.1.1.7.e', 'B.1.617.2.e', 'B.1.1.1.e', 'B.1.1.10.e', 'B.1.1.529.2', 'B.1.617.2.95', 'B.1.1.529.1', 'B.1.617.2.6', 'B.1.1.412.e', 'B.1.1.10.3', 'B.1.1.448.e', 'B.1.617.2.45', 'B.1.617.2.91', 'B.1.1.62.e', 'B.1.1.57.e', 'B.1.1.386.e', 'B.1.1.99.e'}\n",
      "{'B.1.1.529.5.9', 'B.1.617.2.95.e', 'B.1.1.1.2.e', 'B.1.1.529.1.e', 'B.1.1.10.3.e', 'B.1.1.1.9.e', 'B.1.617.2.45.e', 'B.1.617.2.6.e', 'B.1.1.529.2.15', 'B.1.617.2.109.e', 'B.1.617.2.36.e', 'B.1.617.2.38.e', 'B.1.617.2.116.e', 'B.1.1.529.5.6', 'B.1.1.529.1.18', 'B.1.617.2.120.2', 'B.1.1.529.1.12', 'B.1.1.529.1.13', 'B.1.1.529.2.86', 'B.1.1.529.1.21', 'B.1.1.529.2.3', 'B.1.1.529.5.3', 'B.1.1.529.2.12', 'B.1.1.529.1.1', 'B.1.1.529.5.11', 'B.1.617.2.120.e', 'B.1.1.529.4.4', 'B.1.1.1.1.2', 'B.1.1.529.4.1', 'B.1.1.529.2.e', 'B.1.1.529.4.6', 'B.1.1.1.6.e', 'B.1.1.529.1.19', 'B.1.1.529.4.8', 'B.1.1.529.5.1', 'B.1.1.529.4.e', 'B.1.1.529.4.2', 'B.1.617.2.32.e', 'B.1.1.529.1.14', 'B.1.1.529.4.7', 'B.1.617.2.91.e', 'B.1.617.2.122.e', 'B.1.1.529.2.75', 'B.1.1.1.1.e', 'B.1.617.2.99.e', 'B.1.1.529.1.17', 'B.1.1.529.1.15', 'B.1.1.529.3.e', 'B.1.1.529.5.e', 'B.1.1.529.1.10', 'B.1.617.2.46.e', 'B.1.1.529.5.2', 'B.1.617.2.19.e', 'B.1.1.529.2.9', 'B.1.617.2.107.e'}\n",
      "{'B.1.1.529.4.8.e', 'B.1.1.529.2.12.1', 'B.1.1.529.5.3.1', 'B.1.1.529.5.1.e', 'B.1.1.529.4.1.e', 'B.1.1.529.4.7.e', 'B.1.1.529.5.2.22', 'B.1.1.529.1.14.e', 'B.1.1.529.1.1.e', 'B.1.1.529.5.3.e', 'B.1.1.529.2.86.e', 'B.1.1.529.1.12.e', 'B.1.1.529.5.11.e', 'B.1.1.529.1.18.e', 'B.1.1.529.1.19.e', 'B.1.1.529.5.9.e', 'B.1.1.529.2.86.3', 'B.1.1.529.1.21.e', 'B.1.1.529.5.6.e', 'B.1.1.529.5.3.3', 'B.1.1.529.2.75.3', 'B.1.1.529.4.1.1', 'B.1.1.529.2.15.e', 'B.1.1.529.4.4.e', 'B.1.1.529.4.1.8', 'B.1.1.529.1.15.e', 'B.1.1.529.2.3.e', 'B.1.1.529.5.3.5', 'B.1.1.529.5.2.20', 'B.1.1.529.5.2.6', 'B.1.1.529.1.1.14', 'B.1.1.1.1.2.e', 'B.1.1.529.1.17.2', 'B.1.1.529.5.1.23', 'B.1.1.529.4.6.e', 'B.1.1.529.5.2.1', 'B.1.1.529.2.86.2', 'B.1.1.529.4.1.9', 'B.1.1.529.1.10.e', 'B.1.1.529.1.17.e', 'B.1.617.2.120.2.e', 'B.1.1.529.4.2.e', 'B.1.1.529.2.9.e', 'B.1.1.529.1.13.e', 'B.1.1.529.5.2.e'}\n",
      "{'B.1.1.529.2.86.2.e', 'B.1.1.529.1.17.2.e', 'B.1.1.529.4.1.8.e', 'B.1.1.529.5.2.1.28', 'B.1.1.529.5.3.1.5', 'B.1.1.529.5.3.1.e', 'B.1.1.529.5.2.1.7', 'B.1.1.529.5.3.1.3', 'B.1.1.529.2.75.3.4', 'B.1.1.529.5.3.3.e', 'B.1.1.529.5.1.23.e', 'B.1.1.529.2.86.3.e', 'B.1.1.529.5.3.1.1', 'B.1.1.529.2.12.1.e', 'B.1.1.529.4.1.9.e', 'B.1.1.529.1.1.14.e', 'B.1.1.529.5.2.22.e', 'B.1.1.529.5.3.1.13', 'B.1.1.529.5.2.6.5', 'B.1.1.529.5.3.5.e', 'B.1.1.529.5.2.1.5', 'B.1.1.529.5.2.1.e', 'B.1.1.529.5.2.20.e', 'B.1.1.529.4.1.1.e', 'B.1.1.529.5.3.1.7', 'B.1.1.529.5.3.1.8'}\n",
      "{'B.1.1.529.5.2.1.5.e', 'B.1.1.529.5.2.6.5.e', 'B.1.1.529.5.2.1.7.e', 'B.1.1.529.5.3.1.5.e', 'B.1.1.529.5.3.1.13.e', 'B.1.1.529.5.3.1.3.e', 'B.1.1.529.5.3.1.1.1', 'B.1.1.529.5.2.1.28.e', 'B.1.1.529.2.75.3.4.1', 'B.1.1.529.5.3.1.8.e', 'B.1.1.529.5.3.1.7.e', 'B.1.1.529.5.3.1.1.e'}\n",
      "{'B.1.1.529.5.3.1.1.1.2', 'B.1.1.529.5.3.1.1.1.e', 'B.1.1.529.2.75.3.4.1.1', 'B.1.1.529.5.3.1.1.1.1'}\n",
      "{'B.1.1.529.2.75.3.4.1.1.1', 'B.1.1.529.5.3.1.1.1.1.e', 'B.1.1.529.5.3.1.1.1.1.1', 'B.1.1.529.5.3.1.1.1.2.e'}\n",
      "{'B.1.1.529.5.3.1.1.1.1.1.24', 'B.1.1.529.5.3.1.1.1.1.1.12', 'B.1.1.529.5.3.1.1.1.1.1.2', 'B.1.1.529.5.3.1.1.1.1.1.23', 'B.1.1.529.5.3.1.1.1.1.1.1', 'B.1.1.529.2.75.3.4.1.1.1.1', 'B.1.1.529.5.3.1.1.1.1.1.e'}\n",
      "{'B.1.1.529.5.3.1.1.1.1.1.1.67', 'B.1.1.529.2.75.3.4.1.1.1.1.31', 'B.1.1.529.5.3.1.1.1.1.1.24.e', 'B.1.1.529.5.3.1.1.1.1.1.1.10', 'B.1.1.529.5.3.1.1.1.1.1.1.3', 'B.1.1.529.5.3.1.1.1.1.1.1.1', 'B.1.1.529.5.3.1.1.1.1.1.23.e', 'B.1.1.529.5.3.1.1.1.1.1.1.38', 'B.1.1.529.5.3.1.1.1.1.1.2.e', 'B.1.1.529.5.3.1.1.1.1.1.1.e', 'B.1.1.529.5.3.1.1.1.1.1.12.e', 'B.1.1.529.5.3.1.1.1.1.1.1.64'}\n"
     ]
    }
   ],
   "source": [
    "def get_labels_by_parent(X_train, y_train, parent_node):\n",
    "    valid_indices = []\n",
    "    y_train_filtered = []\n",
    "\n",
    "    parent_level = len(parent_node.split('.'))\n",
    "\n",
    "    for i, sample in enumerate(y_train):\n",
    "        hierarchy = sample.split('.')\n",
    "        # More precise check: ensure the sample's hierarchy matches the parent_node exactly at the correct level\n",
    "        if '.'.join(hierarchy[:parent_level]) == parent_node:\n",
    "            valid_indices.append(i)\n",
    "            # Append the full path up to the child node\n",
    "            if len(hierarchy) > parent_level:\n",
    "                child_label = '.'.join(hierarchy[:parent_level + 1])\n",
    "                y_train_filtered.append(child_label)\n",
    "            else:\n",
    "                # Handle cases where the hierarchy length is less than or equal to parent_level\n",
    "                y_train_filtered.append(parent_node)\n",
    "\n",
    "    # Filter X_train according to valid indices\n",
    "    if isinstance(X_train, pd.DataFrame) or isinstance(X_train, pd.Series):\n",
    "        X_train_filtered = X_train.iloc[valid_indices]\n",
    "    else:\n",
    "        X_train_filtered = X_train[valid_indices]\n",
    "\n",
    "    return X_train_filtered, np.array(y_train_filtered)\n",
    "\n",
    "def get_root_labels(X_train, y_train):\n",
    "    # Generate labels for the root level\n",
    "    y_train_root = [sample.split('.')[0] for sample in y_train]  # Root labels are the first element\n",
    "\n",
    "    return X_train, np.array(y_train_root)\n",
    "\n",
    "# Dictionary to hold classifiers for each parent node\n",
    "classifiers = {}\n",
    "\n",
    "# Define the root classifier and train it\n",
    "root_nodes = set(node.split('.')[0] for node in unique_nodes)\n",
    "X_train_root, y_train_root = get_root_labels(X_train, y_train_e)  # Assuming this function is defined correctly\n",
    "\n",
    "if len(np.unique(y_train_root)) > 1:  # Ensure there are at least two classes\n",
    "    clf_root = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    clf_root.fit(X_train_root, y_train_root)\n",
    "    classifiers['root'] = clf_root  # Store the root classifier\n",
    "\n",
    "# Iterate through levels, skipping the last one and starting from the first level\n",
    "for level in range(0, max_level - 1):  # Adjusted to start from 1, since root is already processed\n",
    "    parent_nodes = set('.'.join(node.split('.')[:level + 1]) for node in unique_nodes if len(node.split('.')) > level)\n",
    "    print(parent_nodes)\n",
    "\n",
    "    for parent_node in parent_nodes:\n",
    "        # Generate labels and filter X_train for the current parent node\n",
    "        X_train_parent, y_train_parent = get_labels_by_parent(X_train, y_train_e, parent_node)\n",
    "        \n",
    "        if len(np.unique(y_train_parent)) > 1:  # Check there's more than one class to predict\n",
    "            # Train a classifier for this parent node\n",
    "            clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "            clf.fit(X_train_parent, y_train_parent)  # Use the filtered and correctly labeled training data\n",
    "\n",
    "            # Store the trained classifier, keyed by the parent node\n",
    "            classifiers[parent_node] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246bee01-52b1-465e-9ad5-e0d407a4312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.617': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.10': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.617.2': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.4': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.617.2.120': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.2': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.2.86': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.4.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.1.17': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.2': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.2.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1.1.1.1.1': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.2.75.3.4.1.1.1.1.31': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'B.1.1.529.5.3.1.1.1.1.1.1.38': RandomForestClassifier(n_jobs=-1, random_state=42)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a971a539-46fd-4c13-a005-456abf254b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the DataFrames\n",
    "classifier_predictions_dfs = []\n",
    "\n",
    "# Iterate through each classifier stored in the dictionary\n",
    "for parent_node, clf in classifiers.items():\n",
    "    \n",
    "    # Get the predicted probabilities for the validation set\n",
    "    probabilities = clf.predict_proba(X_val)\n",
    "    \n",
    "    # Retrieve the class labels\n",
    "    class_labels = clf.classes_\n",
    "    \n",
    "    # Create a DataFrame for the current classifier's probabilities\n",
    "    classifier_df = pd.DataFrame(probabilities, columns=class_labels)\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    classifier_predictions_dfs.append(classifier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a3664a-805e-4b42-a798-a7d6f1461a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.concat(classifier_predictions_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6748b-aa27-491e-8768-75d99778b596",
   "metadata": {},
   "source": [
    "### Top Down to Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6462538-7ffc-4e64-94e8-dd375bfb71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_children(predictions_df):\n",
    "    # Extract column names that don't contain '.' indicating they are root children\n",
    "    return [col for col in predictions_df.columns if '.' not in col]\n",
    "\n",
    "def get_children(node, predictions_df):\n",
    "    if node:  # If the node is specified, find its children\n",
    "        prefix = node + '.'\n",
    "        return [col for col in predictions_df.columns if col.startswith(prefix) and col.count('.') == node.count('.') + 1]\n",
    "    else:  # If no node is specified, return root children\n",
    "        return get_root_children(predictions_df)\n",
    "    \n",
    "def navigate_tree_top_down(predictions_df):\n",
    "    results_df = pd.DataFrame(index=predictions_df.index)\n",
    "\n",
    "    for index, row in predictions_df.iterrows():\n",
    "        current_nodes = get_root_children(predictions_df)\n",
    "        predicted_path = []\n",
    "        \n",
    "        while current_nodes:\n",
    "            try:\n",
    "                # Attempt to find the node with the maximum probability\n",
    "                max_conf_node = max(current_nodes, key=lambda node: row[node])\n",
    "                predicted_path.append(max_conf_node)\n",
    "                current_nodes = get_children(max_conf_node, predictions_df)\n",
    "            except ValueError as e:\n",
    "                print(f\"ValueError: {e}\")\n",
    "                print(f\"Index: {index}, Current nodes: {current_nodes}\")\n",
    "                print(f\"Row data: {row}\")\n",
    "                # Optionally, break or continue depending on how you want to handle the error\n",
    "                break\n",
    "\n",
    "        # Assign the last node in the predicted path as the final prediction\n",
    "        results_df.at[index, 'prediction'] = predicted_path[-1] if predicted_path else None\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fc52be-e98b-4735-96a9-41b84280e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9662086055417887\n",
      "F1 Score: 0.9669571041096796\n"
     ]
    }
   ],
   "source": [
    "y_pred = navigate_tree_top_down(predictions_df)['prediction'].str.replace(\".e\", \"\", regex=False)\n",
    "\n",
    "# Evaluate the overall performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')  # Consider weighted if class imbalance is present\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5342b28-a93e-4f3c-88cf-f91fbfada708",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('predictions/LPP.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
